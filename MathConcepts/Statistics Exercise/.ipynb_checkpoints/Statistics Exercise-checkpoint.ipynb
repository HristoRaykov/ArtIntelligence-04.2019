{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Write yor imports here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics Exercise\n",
    "## Statistical Distributions. Properties of distributions. Applications of Probability and Statistics in Computer Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1. Plotting a Single Distribution. Digits in $\\pi$ and $e$\n",
    "We expect that the decimal digits in $\\pi$ and $e$ will be randomly distributed and there's no reason for any digit to dominate over others. Let's verify this.\n",
    "\n",
    "Using an algorithm, the first 10 004 digits of $\\pi$ and $e$ were generated:\n",
    "$$\n",
    "\\pi = 3.(141592 \\dots 5678)5667\n",
    "e = 2.(718281 \\dots 6788)5674\n",
    "$$\n",
    "\n",
    "The 10 000 digits in brackets were counted. You can see the results in `digits.dat`. Each column corresponds to one digit from 0 to 9. The first row is for $\\pi$ and the second row is for $e$.\n",
    "\n",
    "How are these digits distributed? Are the two distributions different?\n",
    "\n",
    "**Note:** The dataset is **not properly formatted** to work easily. You can transpose it. Now, digit counts will be in rows and variables - in columns. \n",
    "```python\n",
    "digits = pd.read_table(\"digits.dat\", header = None).T\n",
    "```\n",
    "\n",
    "You can also specify column names like this:\n",
    "```python\n",
    "digits.columns = [\"pi\", \"e\"]\n",
    "```\n",
    "\n",
    "Also note that **we are not creating the histogram of the distribution**. We already have the counts, we need to plot them. In a sense, the histogram has already been calculated.\n",
    "\n",
    "To do this, we can create a \"bar chart\" (using `plt.bar()`). We have to provide values for the x-axis and y-axis. For the x-axis, we have the numbers 0 through 9 (we can use the *index* of the dataset like this: `digits.index`). For the y-axis, we need to plot the digit counts directly.\n",
    "\n",
    "We can see that even the simplest datasets sometimes need a bit of preprocessing. This is always the case when we're working with data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try something else. Scientists have measured the percentage of silica ($\\text{SiO_2}$, sand / glass) for 22 meteors. You can find it in `silica.dat`. How are these distributed? What is a \"typical\" percentage? Is there such percentage at all?\n",
    "\n",
    "Print the mean, standard deviation (you can use the biased or unbiased formula), skewness and kurtosis of the distribution. What do these numbers tell you? How do they relateto the shape of the distribution? Can you characterize the distribution better? (An idea would be to characterize different parts of it on their own, as if they're different distributions.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2. Categorical Variables. Comparing Categories\n",
    "In addition to numeric variables (like age and salary), in statistics we also use **categorical variables**. These are descriptions of quality (as opposed to quantity). Such variables can be gender, smoker / non-smoker, results of a medical study (healthy / not healthy), colors (red, green, blue), etc. To plot values of categories, we use *bar charts*. Since category names can be long, it's sometimes useful to plot the lines horizontally.\n",
    "\n",
    "<p style=\"color: #d9534f\"><strong>There is a very significant difference between histograms and bar charts. Histograms are used to plot the frequency distribution of one numeric variable. Bar charts are used to plot categorical variables - how each value compares to other values.</strong></p>\n",
    "\n",
    "The dataset `us_budget.dat` contains the figures for the eight main items in the US budget for 1978 and 1979 in billions\n",
    "of dollars.\n",
    "\n",
    "Display the two budgets separately. Use `xlabel()` (or `ylabel()` if your plot is horizontal) to write the names of each category. You can use [this](https://matplotlib.org/examples/pylab_examples/barchart_demo.html) and [this](https://matplotlib.org/examples/pylab_examples/barchart_demo2.html) examples as a guide.\n",
    "\n",
    "Create another variable which shows the difference in budget $\\Delta b = b_{1979} - b_{1978}$. Add this variable to the dataset (find out how). Plot it. How does the budget differ?\n",
    "\n",
    "Since the numbers are different, a better comparison will be if we convert them to percentages of the total budget. Create two more variables for 1978 and 1979 and add them to the dataset. Plot these now. Also plot the difference in percentage, like you did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3. Correlations between Variables. Alcohol and Tobacco Usage\n",
    "The dataset `alcohol_tobacco.dat` shows the average weekly household spending, in British pounds, on tobacco products and alcoholic beverages for each of the 11 regions of Great Britain.\n",
    "\n",
    "Create a scatter plot. Print the correlation coefficient. You can use the **correlation matrix** (find out how).\n",
    "\n",
    "There's a major outlier. Which is it?\n",
    "\n",
    "Remove the outlier from the dataset (find out how). Calculate the correlation coefficient once again. It should be much higher.\n",
    "\n",
    "This example is useful to show what an outlier is, and how an outlier can influence the results of an experiment.\n",
    "\n",
    "**Note:** Be careful with outliers. Sometimes they indicate human error (e.g. human height 1588 cm is obviously wrong) but sometimes they indicate important patterns in the data. Should you remove, replace, or leave them is a difficult question and should be answered separately for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4. Linear Regression, Part 3. Putting It All Together\n",
    "We can now see the entire picture. We started with data points $(x; y)$. Those were related:\n",
    "$$ y = ax + b + \\varepsilon $$\n",
    "\n",
    "We needed to find \"the best\" line which describes the points (and \"disregards\" the random errors). We knew that the best line should lie as close as possible to all points.\n",
    "\n",
    "We model the data with a line with unknown coefficients: $\\hat{y} = \\hat{a}x + \\hat{b}$.\n",
    "\n",
    "We defined the distance between a point $(x, y)$ and the line through $(x, \\hat{y})$ as $d = (y-\\hat{y})^2$. Using this, we defined the total distance as the sum of all distances (normalized  over all points):\n",
    "$$ J = \\frac{1}{N}\\sum(y_i-\\hat{y}_i)^2 = \\frac{1}{N}\\sum\\left(y_i-(\\hat{a}x + \\hat{b})\\right)^2 $$\n",
    "\n",
    "This is our **loss function**. The best line will **minimize** this distance.\n",
    "\n",
    "To minimize the distance, we could use **gradient descent**. This method finds the parameters which correspond to the best line. After that, we're ready. We have now modelled the original data with our line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_noisy_linear_data(a, b):\n",
    "    \"\"\"\n",
    "    Generates a noisy sequence y = ax + b + epsilon\n",
    "    \"\"\"\n",
    "    x = np.linspace(-4, 4, 10)\n",
    "    y = a * x + b\n",
    "    y_error = np.random.randn(len(x))\n",
    "    y += y_error\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_x, data_y = generate_noisy_linear_data(2, 3) # y = 2x + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_loss(x, y, a, b):\n",
    "    \"\"\"\n",
    "    Calculates the loss function for linear regression. \n",
    "    Accepts the original data points (x, y), and the parameters of the line (a, b).\n",
    "    \"\"\"\n",
    "    y_predicted = a * x + b\n",
    "    distances = (y - y_predicted) ** 2\n",
    "    return np.sum(distances) / len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the loss function with several values of a and b\n",
    "for model_a, model_b in [(2, 3), (2, 2.5), (2, 10), (-5, 3)]:\n",
    "    print(\n",
    "        \"Loss function at a = \" + str(model_a) + \", b = \" + str(model_b) + \":\", \n",
    "        calculate_loss(data_x, data_y, model_a, model_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement gradient descent, we need to calculate the partial derivatives of the loss function with respect to $a$ and $b$. We can do this numerically (using a function for numerical differentiation). However, numerical differentiation can be problematic. Also, it's always possible to calculate a derivative of a \"well-behaved function\" analytically. That's why we'd better calculate the derivatives by hand first and input the formulas directly.\n",
    "\n",
    "If you don't want to do this, here are the formulas for the derivatives. Note that they are all scaled by the number of points, like the error function:\n",
    "$$ \\frac{\\partial J}{\\partial a} = -\\frac{2}{N} \\sum x_i\\left(y_i - \\hat{y}\\right) $$\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial b} = -\\frac{2}{N} \\sum \\left(y_i - \\hat{y}\\right) $$\n",
    "\n",
    "where $\\hat{y}_i = ax_i + b$.\n",
    "\n",
    "Note the negative sign. This is because we want to \"move downhill\" on the gradient.\n",
    "\n",
    "After computing the gradients, we need to update the values:\n",
    "$$ a = a - \\alpha \\frac{\\partial J}{\\partial a} $$\n",
    "\n",
    "$$ b = b - \\alpha \\frac{\\partial J}{\\partial b} $$\n",
    "\n",
    "where $\\alpha$ is an \"outside\" parameter (it defines what step size we want to take). It's commonly called **learning rate**.\n",
    "\n",
    "**Bonus:** Compare the vectorized implementation below to an unvectorized implementation, using standard Python. See for example [this one](https://github.com/mattnedrich/GradientDescentExample/blob/master/gradient_descent_example.py). Look at how much cleaner (and probably faster) the code is when using `numpy` and vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def perform_gradient_descent(x, y, a, b, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs one step of gradient descent on the linear regression y = ax + b\n",
    "    using the cost function J\n",
    "    \"\"\"\n",
    "    a_gradient = -2 / len(x) * np.sum(x * (y - (a * x + b)))\n",
    "    b_gradient = -2 / len(y) * np.sum(y - (a * x + b))\n",
    "    \n",
    "    new_a = a - a_gradient * learning_rate\n",
    "    new_b = b - b_gradient * learning_rate\n",
    "    return (new_a, new_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally want to run our gradient descent algorithm. There are many ways to decide when to stop taking new steps but the most common (and easiest) one is to perform it for some fixed iterations, say 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_a = -10\n",
    "model_b = 20\n",
    "\n",
    "for step in range(1001):\n",
    "    model_a, model_b = perform_gradient_descent(data_x, data_y, model_a, model_b, 0.01)\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step\", step, \"a =\", model_a, \"b =\", model_b, \"cost = \", calculate_loss(data_x, data_y, model_a, model_b))\n",
    "print(\"Final line: \" + str(model_a) + \" * x\" + \" + \" + str(model_b)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_x, data_y)\n",
    "model_y = model_a * data_x + model_b\n",
    "plt.plot(data_x, model_y, color = \"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5. Multiple Linear Regression\n",
    "The notion of linear regression can very easily be generalized to more than one variable. Let's inspect a popular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_boston()\n",
    "print(housing.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We usually want to work with the data using `pandas`. The default format is a bit inconvenient. Let's quickly create a new `pd.DataFrame` from the original data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = pd.DataFrame(data = housing.data, columns = housing.feature_names)\n",
    "housing_df[\"Price\"] = housing.target\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All variables except the last one are called **explanatory variables**. The price of the house is our **target variable**.\n",
    "\n",
    "We propose the hypothesis that the price is a linear function of all other predictors, that is:\n",
    "\n",
    "$$ \\text{Price} = \\beta_0 + \\beta_1 \\text{CRIM} + \\beta_2 \\text{ZN} + \\beta_3 \\text{INDUS} + \\dots + \\beta_{13}\\text{LSTAT}  $$\n",
    "\n",
    "This is the same as $y = ax + b$, except $x$ is now a vector, not a single number.\n",
    "\n",
    "How can we attack the problem? We know that the variables are independent, so we can perform many linear regressions:\n",
    "\n",
    "$$ \\text{Price}_{CRIM} = \\beta_{0,\\text{CRIM}} + \\beta_1 \\text{CRIM} $$\n",
    "$$ \\text{Price}_{ZN} = \\beta_{0,\\text{ZN}} + \\beta_1 \\text{ZN} $$\n",
    "$$ \\vdots $$\n",
    "\n",
    "$$ \\text{Price} = \\text{Price}_{CRIM} + \\text{Price}_{ZN} + \\dots + \\text{Price}_{LSTAT} $$\n",
    "\n",
    "All these operations can be vectorized. This means that if we take $x$ and $b$ to be vectors, and $A$ to be a matrix, we can perform all those calculations easily and computationally fast.\n",
    "\n",
    "Of course, we won't bother doing this right now. Feel free to implement this as an exercise.\n",
    "\n",
    "Instead, we'll use a ready model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression = LinearRegression()\n",
    "explanatory_variables = housing_df.drop(\"Price\", axis = 1)\n",
    "regression.fit(explanatory_variables, housing_df.Price)\n",
    "\n",
    "for coefficient in zip(explanatory_variables.columns, regression.coef_):\n",
    "    print(coefficient)\n",
    "print(\"Free term:\", regression.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many questions left. How good is the model? How well can it predict data? Are all variables necessary? Are there better models? We'll leave those unanswered for now. Feel free to explore them but beware: things can get hairy :).\n",
    "\n",
    "**Note:** Usually when we work with data, we want to know more about it before applying any kind of model. This includes cleaning the data and exploring it. In this case we didn't do this only because we wanted to accentuate on the multiple linear regression steps. **In any real situation** when we're presented with an unknown dataset, we have to get acquainted with the data before doing anything else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6. Simulation\n",
    "Another prediction technique based on statistics, is simulation. This means recreating a system's parameters and running the experiment on a computer instead of running it in real life. Simulation can give us many insights. It's useful for prediction, \"what-if\" analysis, etc. It's also very useful if we have very limited \"real experimentation\" resources and want to narrow down our possibilities.\n",
    "\n",
    "Let's see how we can simulate the profit of a grocery shop.\n",
    "\n",
    "The profit is dependent on the customers and what items they buy. Let's assume that the number of customers per months follows a normal distribution with mean 500 and standard deviation 20.\n",
    "\n",
    "$$ C ~ N(500, 20) $$\n",
    "\n",
    "In the shop, there are several items, each having a different popularity. The popularity represents the probability of buying each item.\n",
    "\n",
    "| Item               | Price | Popularity |\n",
    "|--------------------|-------|------------|\n",
    "| Bread              | 0.99  | 0.5        |\n",
    "| Milk               | 2.89  | 0.15       |\n",
    "| Eggs, dozen        | 2.00  | 0.2        |\n",
    "| Chicken fillet, kg | 6.39  | 0.15       |\n",
    "\n",
    "Each customer buys *exactly one* article at random. Each customer will generate an expected profit equal to $\\text{price} . \\text{popularity}$. Total profit: sum of all profits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_customer_profit():\n",
    "    n = np.random.random()\n",
    "    if n <= 0.5:\n",
    "        return 0.99\n",
    "    elif n < 0.65:\n",
    "        return 2.89\n",
    "    elif n <= 0.85:\n",
    "        return 2\n",
    "    else:\n",
    "        return 6.39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation():\n",
    "    days = 1000\n",
    "    profits = []\n",
    "    for day in range(days):\n",
    "        customers = np.floor(np.random.normal(500, 20))\n",
    "        profit = sum([get_customer_profit() for c in np.arange(customers)])\n",
    "        profits.append(profit)\n",
    "    return profits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits = run_simulation()\n",
    "plt.hist(profits, bins = 50)\n",
    "plt.xlabel(\"Profit for \" + str(days) + \" days [$]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can answer questions like:\n",
    "* What's the probability of profit less than \\$1100? \n",
    "* What's the probability of profit between \\$1300 and \\$1400?\n",
    "\n",
    "We can also change our model. Let's suppose now that one customer can take 1, 2 or 3 items, with probabilities 0.5, 0.3 and 0.2 respectively. The picked items are independent. How does this change the distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_customer_profit_many_items(items = 1):\n",
    "    customer_sum = sum([get_customer_profit() for i in range(items)])\n",
    "    return customer_sum\n",
    "\n",
    "def get_total_customer_profit():\n",
    "    n = np.random.random()\n",
    "    if n <= 0.5:\n",
    "        return get_customer_profit_many_items(1)\n",
    "    elif n <= 0.8:\n",
    "        return get_customer_profit_many_items(2)\n",
    "    else:\n",
    "        return get_customer_profit_many_items(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation_many_items():\n",
    "    days = 1000\n",
    "    profits_many_items = []\n",
    "    for day in range(days):\n",
    "        customers = np.floor(np.random.normal(500, 20))\n",
    "        profit = sum([get_total_customer_profit() for c in np.arange(customers)])\n",
    "        profits_many_items.append(profit)\n",
    "    return profits_many_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profits_many_items = run_simulation_many_items()\n",
    "plt.hist(profits_many_items, bins = 50)\n",
    "plt.xlabel(\"Profit for \" + str(days) + \" days [$]\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Comparison of profits: 1 vs 3 items\")\n",
    "plt.hist(profits, bins = 20)\n",
    "plt.hist(profits_many_items, bins = 20)\n",
    "plt.xlabel(\"Profit\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Problem 7. Monte Carlo Simulation\n",
    "One common technique to apply simulations is called **Monte Carlo simulation**. It's similar to the simulation from the previous example. The main idea is to use random sampling to solve deterministic problems.\n",
    "\n",
    "Research what these simulations are. Give examples. Implement at least one case of a Monte Carlo simulation. You can use the following checklist to help with your research and work:\n",
    "* What is a simulation?\n",
    "    * How is simulation used in science?\n",
    "    * Why is a simulation useful?\n",
    "* How are statistics useful in simulation? How can we simulate unknown, random processes?\n",
    "* What is a Monte Carlo simulation (also known as \"Monte Carlo method\")?\n",
    "* A common use of Monte Carlo methods is numeric integration\n",
    "    * Define the problem. Propose the solution. Implement it and test with some common functions\n",
    "    * How does this method compare to other methods, e.g. the trapezoidal rule? Compare the performance (accuracy and time to execute) of both methods\n",
    "* Apply Monte Carlo simulation to a real-life system. There are many examples. You can see [Wikipedia](https://en.wikipedia.org/wiki/Monte_Carlo_method#Applications) or some other resource for inspiration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### * Problem 8. Probabilistic Data Structures\n",
    "A very interesting application of probability in computer science is a kind of data structures which have a probabilistic behaviour. Examples of these are **Bloom filter**, **Skip list**, **Count-min sketch** and **HyperLogLog**.\n",
    "\n",
    "Research how one of these structures works. Or write about many of them, if you wish. You can use the following checklist as a guide:\n",
    "* What is a data structure? \n",
    "* What is a probabilistic data structure?\n",
    "    * Where does the probabilistic behaviour emerge?\n",
    "    * What advantages do these structures provide?\n",
    "* For your chosen structure, how is it constructed?\n",
    "    * What parts do you need? What are the details?\n",
    "* How does the structure work?\n",
    "    * What operations can you do?\n",
    "    * What are the typical probabilities associated with these operations?\n",
    "* Analyze the structure\n",
    "    * Analyze the runtimes for all operations\n",
    "    * Analyze the space usage\n",
    "    * Compare to a similar, non-probabilistic data structure\n",
    "    * What advantages does the new data structure have? What drawbacks do you need to be aware of?\n",
    "* Give at least one example where this structure is useful\n",
    "    * E.g. Bloom filter - spell checkers\n",
    "    * Analyze the use case\n",
    "    * If possible, implement the use case\n",
    "    * Display some metrics (e.g. % conserved space, % reduced time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
